{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982cce07",
   "metadata": {},
   "source": [
    "# ID 5059 Coursework 1\n",
    "John Belcher-Heath (jbh6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa42b20",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The task is to predict the price of a car from a subset of attributes from the Kaggle dataset.\n",
    "\n",
    "I will complete the task following the ML checklist in the book, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. which is:\n",
    "\n",
    "1. Frame the problem\n",
    "2. Get the data\n",
    "3. Explore the data\n",
    "4. Prepare the data\n",
    "5. Explore models\n",
    "6. Fine-tune models\n",
    "7. Present solution\n",
    "8. Launch/maintain\n",
    "\n",
    "However, for the scope of this project 8. will not be needed and solutions will be presented in an external pdf report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3c9e8",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "\n",
    "We want to predict the price of a car (continuos) using a small selection of attributes available to us. This makes the problem a regression problem.\n",
    "\n",
    "Since this is a regression problem the standard performance measure of Root Mean Square Error (referred to as RMSE from now on) will be used:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n(y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "For this measure we are looking for low RMSE. This will mean small residuals and the model is a good fit for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60086da9",
   "metadata": {},
   "source": [
    "# 2. Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64746763",
   "metadata": {},
   "source": [
    "In this section a random slection of entries from one of the large datasets will be obtained and read into a pandas.dataframe to explore. A random selection of the large dataset will be explored since all we are doing is getting to know the data. Having a large amount of data to explore will be time consuming, but having too small (and non random sample) will mean our observations may not be valid. Taking a random sample of a large dataset should give a relatively good representation of the overall dataset, whilst minimising the amount of data requiring to be manipulated.\n",
    "\n",
    "Note when it comes to applying the model I will include a check of the data to make sure our observations on the smaller dataset still hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn | grep -v 'already satisfied'\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd73a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path: str = \"/cs/studres/ID5059/Coursework/Coursework-1/data/2_medium\" # uni\n",
    "folder_path : str = r\"/home/johnbh/personal_git/ID5059_coursework_1/data/3_large\" # Desktop\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    raise FileNotFoundError\n",
    "os.chdir(folder_path)\n",
    "\n",
    "file_names : list = [i for i in glob.glob(\"*.{}\".format('csv'))]\n",
    "\n",
    "    \n",
    "def read_car_data(filepath : str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a filepath and returns the dataframe\n",
    "    :param filepath: The location of the file to read\n",
    "    :return: returns the pandas dataframe\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)#, index_col = \"vin\")\n",
    "\n",
    "original_df: pd.DataFrame = read_car_data(file_names[0])\n",
    "\n",
    "# Clear the maximum number of columns to be displayed, so that all will be visible.\n",
    "pd.set_option('display.max_columns', None)\n",
    "# check data looks roughly okay\n",
    "original_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5ccd2",
   "metadata": {},
   "source": [
    "# 3. Explore the data\n",
    "\n",
    "The data will now be inspected to explore what attributes are available to using the info output. Attributes with large proportion of NAs can start to be identified as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb85c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# original_df: pd.DataFrame = original_df.reset_index(drop=True) # Reindex to make elements easier to quickly access\n",
    "original_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674a45e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore attributes\n",
    "original_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a004f2b",
   "metadata": {},
   "source": [
    "Initial observations from head:\n",
    "\n",
    "- A lot of measurements contain the units, making the non-numerical\n",
    "- Descriptions contain lots of irrelevant information\n",
    "- A few columns seem to represent the same information\n",
    "- Some attributes appear to have lots of NaNs\n",
    "- Multiple ID attributes which can all be dropped\n",
    "- `major_options` is a list which will need parsing somehow\n",
    "- `power` contains all the info of `horsepower`\n",
    "- Lots of irrelevant metadata to drop\n",
    "\n",
    "Let's split the data and assign the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_train: float = 0.6# fraction of data to use to explore\n",
    "\n",
    "train_set, test_set = train_test_split(original_df, test_size = split_train, random_state=314)\n",
    "\n",
    "train_set_index, test_set_index = train_set.index, test_set.index\n",
    "\n",
    "prices = original_df.price.copy() # Takes from test and training, useful later\n",
    "\n",
    "df = train_set.copy() # copy so can recover if needs\n",
    "sample_size: int = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb5e2c",
   "metadata": {},
   "source": [
    "### Start to inspect\n",
    "Firstly, let's drop all attributes from above which have less than 50% non-null values, since including these may negatively effect our model if a majority of entries do not have this attribute. Using them in our model will mean the model is not very general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddec30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop all attributes with less than 50% non-null values\n",
    "df = df.drop(columns=df.keys()[df.count() / sample_size < 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c377163",
   "metadata": {},
   "source": [
    "### Data types correction\n",
    "Some of the attributes appear to have been imported with different datatype, for example `zip code` as `object` not `int64`. This will be due to some integer attributes containing `NaNs`, and since the system has no interpretation for `NaNs` in `integer` types, they are taken as `object` data types instead. \n",
    "\n",
    "To further inspect this, all `object` data types are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd9c95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=object).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676cbe56",
   "metadata": {},
   "source": [
    "From manual inspection there are some attributes that need further inspection to check they have been given the correct type. The first 5 entries are shown below to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac73142",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.select_dtypes(include=object).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f9b38",
   "metadata": {},
   "source": [
    "The only attribute that can be directly converted to an integer is the `dealer_zip`, this is unlikely to provide any additional information that the `lattitude` and `longitutde` will not already give so no need to convert.This is dropped from our dataset below. \n",
    "\n",
    "This inspection has shown that a lot of the measurements have had units included, so these attributes will need to be converted to numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c406eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dealer_zip\n",
    "try:\n",
    "    df = df.drop(columns='dealer_zip')\n",
    "except KeyError:\n",
    "    print(\"Column already dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb76592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_measurement(s: str) -> float:\n",
    "    \"\"\"\n",
    "    Converts the measuremnt with units to a numerical value\n",
    "    :param s: string measurement\n",
    "    :type s: str\n",
    "    :return: the actual numerical value\n",
    "    \"\"\"\n",
    "    if type(s) == str:\n",
    "        s_split: list = s.split(\" \")\n",
    "        try:\n",
    "            return float(s_split[0])\n",
    "        # If cannot convert to dtype, ie NA then return NA\n",
    "        except ValueError:\n",
    "            return float('NaN')\n",
    "    # If already converted to correct format, ie if function accidently run twice\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "cols_to_convert: list = [\"back_legroom\", \"front_legroom\", \"fuel_tank_volume\", \"height\", \"length\", \n",
    "                         \"maximum_seating\", \"wheelbase\", \"width\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to get numerical data from the string measurements\n",
    "df[cols_to_convert] = df[cols_to_convert].applymap(convert_measurement)\n",
    "df[cols_to_convert] = df[cols_to_convert].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c944903",
   "metadata": {},
   "source": [
    "It is important to note that the attributes `power` and `torque` contain numerical data, but they cannot be simply convert at this point but will be saved for later.\n",
    "\n",
    "Next, let's drop all the irrelevant meta data which won't be helpful with our model and will instead just increase the complexity which could lead to overfitting. For example the `description`, `interior color`, `exterior color`, `vin` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd849e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['description', 'interior_color', 'exterior_color', \n",
    "                      'main_picture_url', 'model_name', 'sp_name', 'transmission_display',\n",
    "                      'trim_name', 'trimId', 'vin', 'sp_id', 'listing_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b57844",
   "metadata": {},
   "source": [
    "### Fixing duplicates part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339cac0",
   "metadata": {},
   "source": [
    "It is easy to see that `engine_cylinders` and `engine_type` appear to be duplicate. Similarly so do `wheel_system` and `wheel_system_display`, as well as `make_name` and `franchise_make`.\n",
    "\n",
    "Before dropping one of each of these, the data will be further inspected to make sure that there's no discrepancy between the two in the wider data set (i.e. not just in the head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engine = df[['engine_cylinders', 'engine_type']]\n",
    "df_engine[np.logical_xor(df_engine.engine_cylinders.isna(), df_engine.engine_type.isna())].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e80067",
   "metadata": {},
   "source": [
    "So above tells us that all entries with attributes are identical in being either NA or not, so dropping one of these attributes means no information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='engine_cylinders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235f274",
   "metadata": {},
   "source": [
    "For the `wheel_system` and `wheel_system_display`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc477a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_wheel = df[['wheel_system', 'wheel_system_display']]\n",
    "df_wheel[np.logical_xor(df_wheel.wheel_system.isna(), df_wheel.wheel_system_display.isna())].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86c5f3",
   "metadata": {},
   "source": [
    "The above implies that both attributes provide the same information for the cars. Hence deciding which to drop is irrelevant. I will choose to drop the `wheel_system_display` since wheel system has a nice short appriviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20343320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='wheel_system_display')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c0eb2",
   "metadata": {},
   "source": [
    "Finally for make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_make = df[['make_name', 'franchise_make']]\n",
    "df_make[np.logical_xor(df_make.make_name.isna(), df_make.franchise_make.isna())].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3add77e",
   "metadata": {},
   "source": [
    "From this we can see that the `make_name` has more information than the `franchise_make`, hence the `franchise_make` is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='franchise_make')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad8fa8",
   "metadata": {},
   "source": [
    "### Fixing duplicates part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41cc41b",
   "metadata": {},
   "source": [
    "For part 2, these duplicates data may need to be extracted then compared, before just dropping attributes.\n",
    "\n",
    "Let's inspect the engine data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[np.logical_xor(df.engine_displacement.isna(), df.horsepower.isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480b0a6",
   "metadata": {},
   "source": [
    "So, luckily `horsepower` and `power` do give the same information so one can be dropped arbitrarily. As horsepower is already numerical, `power` will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b26678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('power', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deeb060",
   "metadata": {},
   "source": [
    "There is also another useful attribute of RPM which could help to distinguish between performance cars with large horsepower and 4x4 with the same, but there may be too many NAs for this attribute to use this metric, let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.horsepower.count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87118ac",
   "metadata": {},
   "source": [
    "So from above we can see that only around 5% have no `horsepower` attribute. For these remaining entries we will consider how many have engine size attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a2618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[(df['horsepower'].isna() & df['engine_type'].isna())]) / sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ REDO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21553600",
   "metadata": {},
   "source": [
    "Now there is only a small amount of cars with neither `horsepower` or `engine_type` attribute. All these entries will simply take the overall median for `horsepower`.\n",
    "\n",
    "The `horsepower` for all cars will be assigned using the following:\n",
    "\n",
    "- if the car has `horspower` asigned pass\n",
    "- elif the car has `engine_type` assign median for that type\n",
    "- else assign the overall median for `horsepower`\n",
    "\n",
    "Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['horsepower']] = df[['horsepower', 'engine_type']].groupby('engine_type').transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb21fa",
   "metadata": {},
   "source": [
    "**Note: this is an imputation step so we don't want to use horsepower as a way to fill other NaNs as this could lead to too muhc creation of data and introduce bias**\n",
    "\n",
    "Let's examine the improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed63eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.horsepower.count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a221cac",
   "metadata": {},
   "source": [
    "Now for the final step of assigning the last na just the average of all the horsepowers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1926a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(df[['horsepower']])\n",
    "\n",
    "df[['horsepower']] = imputer.transform(df[['horsepower']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e6a42",
   "metadata": {},
   "source": [
    "Let's see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.horsepower.count() / sample_size, df.horsepower.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f67186",
   "metadata": {},
   "source": [
    "Everything looks all good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9f578",
   "metadata": {},
   "source": [
    "### Object type attributes\n",
    "Now we have removed some of the duplicates and corrected some of the data type issues the `object` type attributes will be properly explored now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=object).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347a507",
   "metadata": {},
   "source": [
    "First let's see if any of the attributes have any blaring issues with NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b53ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=object).count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b3b8a",
   "metadata": {},
   "source": [
    "Clerly some of the attributes are not suitable to use since they have a low number of entries. Any object attributes with less than 80% entries are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=df.select_dtypes(include=object).loc[:, df.select_dtypes(include=object).count() / sample_size < 0.8].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80c1e0",
   "metadata": {},
   "source": [
    "This leaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95142e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=object).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fce31",
   "metadata": {},
   "source": [
    "Since we have `daysonmarket` attribute the `listed_date` can be dropped. Additionally, `city` can assumed to have minimal effect since most cities can be assumed to have a diverse range of individuals with varying wealth and cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['city', 'listed_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f736c8e",
   "metadata": {},
   "source": [
    "`torque` could be useful but there is too few entries (see below) for it and it is not recorded elsewhere (like `horsepower` recorded in `power` and `engine_size`). Hence I will not use this attribute for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00811b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.torque.count() / sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='torque')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241eaf8b",
   "metadata": {},
   "source": [
    "For major options, since there is so much variabilty from visual inspection of naming of products, the number of major of features will be used instead. The actual usefulness of this will be explored later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b38de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.major_options = df.major_options.apply(lambda x: len(x.split(\",\")) if type(x) == str else \"NaN\").astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858a3ea",
   "metadata": {},
   "source": [
    "For the remaining attributes, these will be used as categorical attributes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66122534",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_categorical_attributes : list = df.select_dtypes(include=object).keys().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0514b0",
   "metadata": {},
   "source": [
    "### Explore the bool values\n",
    "Next let's explore the boolean attributes available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=bool).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a24dd",
   "metadata": {},
   "source": [
    "Both these could be useful, let's see if there's any issues with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364b7f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=bool).count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eff84e",
   "metadata": {},
   "source": [
    "Fantastic! Both these attriubtes have no NaNs so can be used straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_boolean_attributes : list = ['franchise_dealer', 'is_new']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493d7be",
   "metadata": {},
   "source": [
    "### Exploring the numerical attributes\n",
    "\n",
    "Now the qualitative attributes have been dealt with it's time for the quantiative attributes.\n",
    "\n",
    "Let's explore all the numerical attributes with an actual numerical meaning(index or listing_id have no meaning numerically). Attributes with no numerical meaning our dropped below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5357688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspection to see which numerical but non-relevant attributes need to be dropped\n",
    "df.select_dtypes(include=[np.int64, np.float64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2cf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "df_numerical = df.select_dtypes(include=[np.int64, np.float64])\n",
    "df_numerical.hist(figsize=(16,20), bins=30)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778a9db",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Both Fuel economy attributes appear to be normally distributed with a slight skew\n",
    "- Majority of cars do not stay on the market for a long duration, mostly less than a couple of months. Some may be above a large amount so these may need to be removed to not skew data.\n",
    "- Engine displacement doesn't appear to have any obvious standard distribution\n",
    "- Horsepower appears to have a normal distribution around 200hp with a standard deviation of around 50hp\n",
    "- Lattitude is as expected all grouped together around 39 to 44 \n",
    "- longitutde is split into two peaks, most likely corresponding to central US and alaska\n",
    "- Milegae of most cars is grouped mostly around 0 and fewer cars with higher mileage, as would be expected\n",
    "- owner count has a modal of 1, again as to be expected\n",
    "- Most cars prices are group around the same order of magnitutde. Howeever some extremes are seen. A logarithmic transformation may need to be considered later.\n",
    "- Seller ratings appear to be skew negatively towards the higher end\n",
    "- Majority of cars are from the last 15 years\n",
    "- Modal max seats is 5\n",
    "\n",
    "It is clear as well that some of the bins are very sparse so will need coarser bins with labels for our model later to make sure our training set and test set have similar distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd11af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[np.float64, np.int64]).count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c9caf",
   "metadata": {},
   "source": [
    "Firstly it is clear to see there is no issue with NAs in the attributes: `daysonmarket`, `lattitude`, `longitude`, `price`, `savings_amount` and `year` (as well as `horsepower` after the fix above). Using contextual knowledge all these attributes (excl `price` as this is being compared to) will likely be useful in predicting the `price` attribute so will be used. \n",
    "\n",
    "Looking at the list of other attributes available with a low number of non-nulls. The additional attributes I believe may effect the `price` and want to explore more are:\n",
    "\n",
    "- `city_fuel_economy` and `highway_fuel_economy` - useful metric of car performance, more powerful and expensive cars likely to have lower fuel efficiency\n",
    "- `fuel_tank_volume` - bigger more expensive cars likely to have a large fuel tank, hence useful metric\n",
    "- `engine_displacement` and `horsepower` (and `power` which will be used to get na values) - all similar/the same metrics for how powerful a car is\n",
    "- `major_options` - more expensive cars tend to have more options\n",
    "- `mileage` - more miles done the less it is valued generally\n",
    "- `seller_rating` - If a seller has a better rating people may pay more than if they were to go to a seller with a poor rating.\n",
    "- `length` and `width` - A measure of the size of the car. Large cars tend to be more expensive. E.g. sports cars are very wide generally.\n",
    "\n",
    "I have chosen not to include `owner_count` since there are too few entries for this attribute. Additonally `maximum_seating` is also excluded, although small number of non-nulls, this is since cars seating will have little effect on price. Think two seat sports cars and a smart car, or a 5 seat corsa and a 5 seat golf.\n",
    "\n",
    "To explore these options there is some transformation required to remove any skew by the extreme values, also to reduce the complexity of the model.\n",
    "\n",
    "### Attribute transformation\n",
    "From the graphs above some attributes we have chosen to explore further need transforming so that the distribution of the training set and test set are similar. To do this the function below will be used.\n",
    "\n",
    "**The function will be demonstrated but not applied to the data yet since there are NAs that need filling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbbcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO TWEAK\n",
    "def transform_bins(pds: pd.Series, bins, min_val = None, max_val = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Function to transform a continuous series with sparse data to a categorical attribute with full bins.\n",
    "    The absolute max is always 0 and inf to make sure all data is captured.\n",
    "    :param pds: original cts data\n",
    "    :param bins: number of bins in resultant series (note this is how many will be attempted to be created)\n",
    "    :min_val: starting value for main section of the bins\n",
    "    :max_val: ending value for main section of the bins\n",
    "    :return: transformed series\n",
    "    \"\"\"\n",
    "    bins -=1\n",
    "    if min_val is not None and max_val is not None: \n",
    "        cuts: list = np.append(np.linspace(min_val, max_val, bins), np.array([np.inf])).tolist()\n",
    "        cuts.insert(0,0)\n",
    "    else:\n",
    "        cuts: list = np.append(np.linspace(pds.quantile(0.025), pds.quantile(0.975), bins), np.array([np.inf])).tolist()\n",
    "        cuts.insert(0, 0)\n",
    "        \n",
    "    # Drop any duplicates, ie if 0 included twice\n",
    "    cuts = list(dict.fromkeys(cuts))\n",
    "    labels: list = [str(i) for i in range(len(cuts)-1)]\n",
    "    # include_lowest needed to make sure if values are 0 they're still given a label\n",
    "    return pd.cut(pds, bins=cuts, labels=labels, include_lowest=True).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14e531",
   "metadata": {},
   "source": [
    "The attributes needing to be transformed are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb29ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_attributes: list = [\"city_fuel_economy\", \"highway_fuel_economy\",\"daysonmarket\", \"fuel_tank_volume\", \n",
    "                              \"mileage\", \"savings_amount\", \"year\", \"major_options\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736618f",
   "metadata": {},
   "source": [
    "The function will be applied in a uniform way with 30 bins for each first, these will then be inspected to see if more detailed transformation may be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47086a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_attr: pd.DataFrame = df[transform_attributes].apply(lambda x: transform_bins(x, bins=30))\n",
    "transformed_attr.hist(figsize=(16,16))\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0c1f8",
   "metadata": {},
   "source": [
    "These distributions look much better than before. However there may be a slight issue with `savings_amount` and `city_fuel_economy`. For this one different min, max and bins need to be used. Using contextual knowledge the following conversions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df45a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_attr[['city_fuel_economy']] = df[['city_fuel_economy']].apply(lambda x: transform_bins(x, bins=5, min_val=18, max_val=28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_attr[['savings_amount']] = df[['savings_amount']].apply(lambda x: transform_bins(x, bins=5, min_val=100, max_val=3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_attr.hist(figsize=(16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4dd3c2",
   "metadata": {},
   "source": [
    "These look much better than before.\n",
    "\n",
    "After all the exploratory analysis a list of attributes which are hopefully correlated to the `price` attribute have been identified. But before preperation let's take a look at the correlation between the numerical attributes and the `price` to maybe eliminate some attributes, reducing the complexity.\n",
    "\n",
    "### Explore correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_numerical_attributes : list = ['daysonmarket', 'latitude', 'longitude', 'price', 'savings_amount', 'year', 'horsepower', 'city_fuel_economy', \n",
    "'highway_fuel_economy', 'fuel_tank_volume', 'engine_displacement', 'major_options', 'mileage', 'seller_rating',\n",
    "'length', 'width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96eaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = df.select_dtypes(include=[np.float64, np.int64])[['daysonmarket', 'latitude', 'longitude', 'price', 'savings_amount', 'year', 'horsepower', 'city_fuel_economy', \n",
    "'highway_fuel_economy', 'fuel_tank_volume', 'engine_displacement', 'major_options', 'mileage', 'seller_rating',\n",
    "'length', 'width', 'wheelbase']]\n",
    "# abs taken as don't care if posotive or negative effect\n",
    "corr_series = abs(df_numerical.drop(\"price\", axis=1).apply(lambda x: x.corr(df_numerical.price)))\n",
    "corr_series.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7eca33",
   "metadata": {},
   "source": [
    "Clearly some the attributes left don't have much of a correlation\n",
    "Now let's choose all attributes with a correlation of more than 0.25 and use some of our contextual knowledge to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced1279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_series[corr_series > 0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e46673",
   "metadata": {},
   "source": [
    "All these attributes seem to make logical sense. One attribute that could be removed is one of `wheelbase` or `length` since they represent different ways to measure the length of a car. Since wheelbase has the higher correlation, `length` will be dropped. Let's inspect the above attributes in more detail. Any attributes we had intially chosen but have are not included above will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4957a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_numerical_attributes = corr_series[corr_series > 0.25].keys().tolist()\n",
    "chosen_numerical_attributes.remove('length')\n",
    "chosen_numerical_attributes.append('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc72a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[chosen_numerical_attributes].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6674f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df[chosen_numerical_attributes], figsize=(15,15))\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fe0b4",
   "metadata": {},
   "source": [
    "Inspecting the `price` row (or column), `horsepower` and `mileage` have the stongest correlation as to be expected. \n",
    "`wheelbase` and `width` appear to have similar correlation to price, which is to be expected by them being a measurement of size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99497b46",
   "metadata": {},
   "source": [
    "## 4. Prepare data\n",
    "\n",
    "Now the intial exploration and some of the transformation needed have been identified. The data can start to be prepared.\n",
    "\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "Seperate the labels from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96424b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prices = original_df[[\"price\"]].copy() # Takes from test and training, useful later\n",
    "#prices.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39430c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop price from dataframe\n",
    "#df = df.drop(columns='price')\n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d093961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_attributes : list = chosen_numerical_attributes + chosen_categorical_attributes + chosen_boolean_attributes\n",
    "chosen_attributes.remove('price')\n",
    "df = df[chosen_attributes].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a82c8",
   "metadata": {},
   "source": [
    "Check out any remaining data values which are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f64033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60cd40",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "Let's fix the null values.\n",
    "\n",
    "Now we want to start thinking about making our pipeline, let's make some custom Imputers to act on our data. Note the imputer created will be used in place of the process for `horsepower` for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f39502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class WithinGroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, group_var):\n",
    "        self.group_var = group_var\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for col in X_.drop(self.group_var, axis=1).columns:\n",
    "            if X_[col].dtypes == 'float64':\n",
    "                # For float types impute with median of group or overall if not available\n",
    "                X_.loc[(X[col].isna()) & X_[self.group_var].notna(), col] = X_[self.group_var].map(X_.groupby(self.group_var)[col].median())\n",
    "                X_[col] = X_[col].fillna(X_[col].median())\n",
    "            if X_[col].dtypes == 'object':\n",
    "                # For object types impute with mode of group or overall if not available\n",
    "                X_.loc[(X[col].isna()) & X_[self.group_var].notna(), col] = X_[self.group_var].map(X_.groupby(self.group_var)[col].agg(pd.Series.mode))\n",
    "                overall_mode = str(X_[self.group_var].mode().tolist()[0])\n",
    "                # Library has issues with object atttributes and NaN so need to replace them with np.NaN explciitly\n",
    "                X_.loc[:, col] = X_[col].fillna(np.nan).replace(np.nan, overall_mode)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddb344",
   "metadata": {},
   "source": [
    "One important thing for imputations I have done is to make sure that any imputations are done from the raw data only, and not from other imputations. This reduces any bias introduced. So for example if the mode of groups is assigned to an attribute, the groups the cars belong to are not found by imputation themselves.\n",
    "\n",
    "Similar to what happened for `horsepower `, `wheelbase` and `width` can assumed to be similar measures of a cars size and hence the `body_type` can be used as a proxy. This is chosen for the high number of non-null values. For any which don't have the `body_type` attribute the overall average will be used. For this imputation we will use the cust `WithinGroupImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_group_imp = WithinGroupImputer(group_var='body_type')\n",
    "df[['wheelbase']] = body_group_imp.fit_transform(df[['wheelbase', 'body_type']])[['wheelbase']]\n",
    "df[['width']] = body_group_imp.fit_transform(df[['width', 'body_type']])[['width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5111d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check everything worked\n",
    "df[['wheelbase', 'width']].count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1115e6",
   "metadata": {},
   "source": [
    "For `mileage`, `year` will be used as a proxy, with the median from that year assigned, median is used as should be a nice symmetrical distribution for each year. An example to support this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.year == 2015].mileage.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa16528",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_group_imp = WithinGroupImputer(group_var='year')\n",
    "df[['mileage']] = year_group_imp.fit_transform(df[['mileage', 'year']])[['mileage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14534474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check everything worked\n",
    "df.mileage.count() / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49121861",
   "metadata": {},
   "source": [
    "For `major_options` the median will be taken, since the number of different possible options is low and a central discrete metric is desried. For this a `SimpleImputer` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654541cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "df[['major_options']] = median_imp.fit_transform(df[['major_options']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5b639",
   "metadata": {},
   "source": [
    "For `wheel_system` the `body_type` will be used as a proxy, and the mode for the given body type will be taken as the value. Note this imputation is done before the imputation of `body_type` to reduce inducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525871e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['wheel_system']] = body_group_imp.fit_transform(df[['wheel_system', 'body_type']])[['wheel_system']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545212a",
   "metadata": {},
   "source": [
    "For `fuel_tank_volume` this is likely proportional to the type of car, so the median of the `fuel_tank_volume` for the `body_type` can be used as proxy. Think about hatchbacks vs trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa553275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['fuel_tank_volume']] = body_group_imp.fit_transform(df[['fuel_tank_volume', 'body_type']])[['fuel_tank_volume']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef0656",
   "metadata": {},
   "source": [
    "Similarly for `transmission` as few entries missing, the mode will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df[['transmission']] = mode_imp.fit_transform(df[['transmission']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25838506",
   "metadata": {},
   "source": [
    "For `fuel_type` let's examine the attribute in more detial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06486bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.fuel_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8a9e7",
   "metadata": {},
   "source": [
    "Clearly an overwhelming majority is Gasoline, so let's fill all NaNs with Gasoline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fuel_type'] = df['fuel_type'].fillna('Gasoline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403a543",
   "metadata": {},
   "source": [
    "For `engine_displacement` the attribute is given the median for an entry with a given `engine_type`, then filled with the overall median if no `engine_type` is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_group_imp = WithinGroupImputer(group_var='engine_type')\n",
    "df[['engine_displacement']] = engine_group_imp.fit_transform(df[['engine_displacement', 'engine_type']])[['engine_displacement']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b23c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## COME BACK TO THIS #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ea859",
   "metadata": {},
   "source": [
    "Now let's deal with `engine_type`. Let's take a closer look at the options available a bit closer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106769ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.engine_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c926f",
   "metadata": {},
   "source": [
    "Inital thoughts are there a lot of different categories so this may be a difficult attribute to fix. Let's explore the the relation between `make_name` and `engine_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('make_name')\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "groups.engine_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add max rows back\n",
    "pd.set_option(\"display.max_rows\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03204cfc",
   "metadata": {},
   "source": [
    "From the above with visual inspection if the mode for a given `make_name` is taken as the `engine_type` this should give a good imputation. Logically most manufactures will mass produce one or similar engine types for all vehicles to reduce production costs. Although there is some outliers in this since there is only a few to fill, this simple method is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_name_imp = WithinGroupImputer(group_var='make_name')\n",
    "df[['engine_type']] = make_name_imp.fit_transform(df[['engine_type', 'make_name']])[['engine_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407b0fb",
   "metadata": {},
   "source": [
    "Let's check this worked okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24dbbd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.engine_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037199c",
   "metadata": {},
   "source": [
    "So some entries have been given an empty array instead of a scalar. This may be due to some brands having only one car which have NaNs for the `engine_type`. Let's take a look deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['engine_type', 'make_name']].groupby('make_name').agg(pd.Series.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd40da",
   "metadata": {},
   "source": [
    "That appears to be the case. \n",
    "\n",
    "So with these, instead of making the transformer any more complicated, we will just apply an extra step to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df40de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['engine_type'].str.len() == 0\n",
    "overall_mode = df['engine_type'].mode()[0]\n",
    "df.loc[mask, 'engine_type'] = overall_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f6eff",
   "metadata": {},
   "source": [
    "For `body_type` since there is very few entries missing, the mode for the attribute would make sense for this catagorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35598a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['body_type']] = mode_imp.fit_transform(df[['body_type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd939597",
   "metadata": {},
   "source": [
    "Let's check all the data is looking good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a7636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.keys()[df.isna().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786043d",
   "metadata": {},
   "source": [
    "Fantastic, all is good!\n",
    "\n",
    "Now all NaNs have been dealt with let's see if the transformations discussed earlier still need applying. The only attributes from before that we transformed left are:\n",
    "- `fuel_tank_volume` \n",
    "- `mileage`\n",
    "- `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31690f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2006e",
   "metadata": {},
   "source": [
    "`year` and `mileage` appear to be the only ones that still need transforming from before. Let's do it same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba220951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_attributes: list = ['mileage', 'year', 'major_options']\n",
    "df[transform_attributes] = df[transform_attributes].apply(lambda x: transform_bins(x, bins=30))\n",
    "df[transform_attributes].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7e4dc",
   "metadata": {},
   "source": [
    "These look much better than before.\n",
    "\n",
    "The next step is to encode the categorical data to be meaning full.\n",
    "\n",
    "So for all these attribute there is no intrinsic ranking. Although `engine_type` could be, there is no factual way of saying which is 'better'. So, one-hot-encoding will be used,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "categorical_data : list = df.select_dtypes(include='object').keys().tolist()\n",
    "df_categorical_encoded = one_hot_encoder.fit_transform(df[categorical_data].astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f2fcd",
   "metadata": {},
   "source": [
    "Let's do the same for the boolean attributes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_data: list = df.select_dtypes(bool).keys().tolist()\n",
    "df_bool_encoded = one_hot_encoder.fit_transform(df[bool_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6abde",
   "metadata": {},
   "source": [
    "Final step for our data is to scale the numerical features so the algorithgms work well. We'll go with the standardisation scaling since most of the data does not have massively large tails. The only ones that may pose an issue are mileage and year, since they do have longer tails than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f018c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_data: list = df.select_dtypes(include=[np.int64, np.float64]).keys().tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_numerical_encoded = pd.DataFrame(scaler.fit_transform(df[numerical_data]), columns = numerical_data)\n",
    "df_numerical_encoded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8a4f5",
   "metadata": {},
   "source": [
    "Let's check they look okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90bf24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_numerical_encoded.hist(figsize=(16,16))\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4161c0",
   "metadata": {},
   "source": [
    "They all look good. Let's carry on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c302b26",
   "metadata": {},
   "source": [
    "### Transformation pipelines\n",
    "Now we have the data preproccessed in the desired form, let's turn this into a pipeline.\n",
    "\n",
    "Firstly, let's get the names of all the attributes we need from the dataset. And print the head of the orignial `train_set` so the index of each attribute can be seen. This is need for some section of the pipeline as the data is passed as numpy arrays, so all index and attributes need to match correctly.*\n",
    "\n",
    "*We don't worry about this before as the columns are assigned directly one at a time so numpy form is no issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51782f32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attributes : list = df.keys().to_list()\n",
    "print(attributes)\n",
    "train_set[attributes].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ce0c8",
   "metadata": {},
   "source": [
    "Next let's inspect the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12248112",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data, categorical_data, bool_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributes = numerical_data + categorical_data + bool_data\n",
    "all_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3fd3d0",
   "metadata": {},
   "source": [
    "For numerical data we need to remeber that `year`, `major_options` and `mileage` are in indicies 0, 4 and 5 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class ExctractAttributesTransform(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cols_to_convert = ['fuel_tank_volume', 'wheelbase', 'width']\n",
    "        self.cols_to_summarise = 'major_options'\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[self.cols_to_convert] = X_[self.cols_to_convert].applymap(convert_measurement).astype(np.float64)\n",
    "        X_[self.cols_to_summarise] = X_[self.cols_to_summarise].apply(lambda x: len(x.split(\",\")) if type(x) == str else \"NaN\").astype(np.float64)\n",
    "        return X_\n",
    "\n",
    "class WithinGroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, group_var, attr):\n",
    "        self.group_var = group_var\n",
    "        self.attr = attr\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for col in self.attr:\n",
    "            if X_[col].dtypes == 'float64':\n",
    "                # For float types impute with median of group or overall if not available\n",
    "                X_.loc[(X_[col].isna()) & X_[self.group_var].notna(), col] = X_[self.group_var].map(X_.groupby(self.group_var)[col].median())\n",
    "                X_[col] = X_[col].fillna(X_[col].median())\n",
    "            if X_[col].dtypes == 'object':\n",
    "                # For object types impute with mode of group or overall if not available\n",
    "                X_.loc[(X_[col].isna()) & X_[self.group_var].notna(), col] = X_[self.group_var].map(X_.groupby(self.group_var)[col].agg(pd.Series.mode))\n",
    "                overall_mode = str(X_[self.group_var].mode().tolist()[0])\n",
    "                # Library has issues with object atttributes and NaN so need to replace them with np.NaN explciitly\n",
    "                X_.loc[:, col] = X_[col].fillna(np.nan).replace(np.nan, overall_mode)\n",
    "        return X_\n",
    "    \n",
    "class columnDropperTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,columns):\n",
    "        self.columns=columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self,X):\n",
    "        return X.drop(self.columns,axis=1)\n",
    "\n",
    "class dataFix(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        mask = X_['engine_type'].str.len() == 0\n",
    "        overall_mode = X_['engine_type'].mode()[0]\n",
    "        X_.loc[mask, 'engine_type'] = overall_mode\n",
    "        return X_.astype(str) # need to convert for encoder\n",
    "    \n",
    "class transformNumericalAttributes(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Indexes corresponding to year, mileage and major_options\n",
    "        self.transform_attributes_index = [0, 4, 5]\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        df = pd.DataFrame(X_[:, self.transform_attributes_index]).apply(lambda x: transform_bins(x, bins=30))\n",
    "        X_[:, self.transform_attributes_index] = df\n",
    "        return X_\n",
    "    \n",
    "numerical_pipeline = Pipeline([\n",
    "        ('extract_numericals', ExctractAttributesTransform()),\n",
    "        ('impute_num_body_groupby', WithinGroupImputer('body_type', ['wheelbase', 'width', 'fuel_tank_volume'])),\n",
    "        ('impute_num,_year_groupby', WithinGroupImputer('year',  ['mileage'])),\n",
    "        ('impute_num_engine_groupby', WithinGroupImputer('engine_type', ['engine_displacement', 'horsepower'])),\n",
    "        ('drop_grouping_attr', columnDropperTransformer(columns=['body_type', 'engine_type'])),\n",
    "    \n",
    "        # Fills any remaining numerical NaN with mdeian, should only apply tomajor_options\n",
    "        # Note this step returns a numpy array so must use index from now on\n",
    "        ('basic_impute', SimpleImputer(strategy='median')),\n",
    "        ('transform_skewed', transformNumericalAttributes()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('impute_cat_body_groupby', WithinGroupImputer('body_type', ['wheel_system'])),\n",
    "    ('impute_cat_make_groupby', WithinGroupImputer('make_name', ['engine_type'])),\n",
    "    ('fixes', dataFix()),\n",
    "    # Basic mode for transmission, body_type and fuel_type\n",
    "    ('basic_mode_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('encoding', OneHotEncoder())\n",
    "])\n",
    "\n",
    "bool_pipeline = Pipeline([\n",
    "    ('encoding', OneHotEncoder())\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"numerical\", numerical_pipeline, (numerical_data + ['body_type', 'engine_type'])),\n",
    "    (\"categorical\", categorical_pipeline, categorical_data),\n",
    "    (\"bool\", bool_pipeline, bool_data)\n",
    "])\n",
    "\n",
    "prepared_data = full_pipeline.fit_transform(train_set[all_attributes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67282d",
   "metadata": {},
   "source": [
    "Let's have a quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(prepared_data.toarray()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef844938",
   "metadata": {},
   "source": [
    "Clearly there is a lot of attribute, this may mean the model takes a while to fit, it could be worth reducing attributes in the future to reduce training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae6dfa",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "Since we are runnning it on a home computer, I will purely consider the first 7 attributes and last 2 which correspond to the numeric attributes and the two boolean attributes.\n",
    "!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71da83",
   "metadata": {},
   "source": [
    "### Encoding the data\n",
    "Now the data set is ready to be encoded correctly for the models. !!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad8f10",
   "metadata": {},
   "source": [
    "## 5. Exploring models \n",
    "### Linear regression model\n",
    "Since `price` is a numerical attribute a linear regression model makes sense. Let's try this model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0197abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared_reduced = prepared_data[:,[0,1,2,3,4,5,6,7,-1,-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(df_prepared_reduced, prices[train_set_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779e7e5",
   "metadata": {},
   "source": [
    "Let's sample some random test data from the `test_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_set.sample(30)\n",
    "pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964590fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = prices[test_data.index]\n",
    "pd.DataFrame(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91757990",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_prepared = full_pipeline.fit_transform(test_data)[:,[0,1,2,3,4,5,6,7,-1,-2]]\n",
    "pd.DataFrame(test_data_prepared.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88793ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = linear_regression.predict(test_data_prepared).round()\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ca9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression_price_predictions = linear_regression.predict(test_data_prepared)\n",
    "linear_regression_mse = mean_squared_error(test_labels, linear_regression_price_predictions)\n",
    "linear_regression_rmse = np.sqrt(linear_regression_mse)\n",
    "np.round(linear_regression_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(linear_regression_rmse / test_labels.median(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10df2b",
   "metadata": {},
   "source": [
    "### Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_regressor = DecisionTreeRegressor(random_state=314)\n",
    "tree_regressor.fit(df_prepared_reduced, prices[train_set_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015ded5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26344ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "K = 10\n",
    "\n",
    "tree_regressor_scores = cross_val_score(tree_regressor, df_prepared_reduced, prices[train_set_index],\n",
    "                         scoring=\"neg_mean_squared_error\", cv=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_regressor_rmse_scores = np.sqrt(-tree_regressor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", np.round(scores))\n",
    "    print(\"Mean:\", np.round(scores.mean()))\n",
    "    print(\"Standard deviation:\", np.round(scores.std()))\n",
    "\n",
    "display_scores(tree_regressor_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b079be",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_scores = cross_val_score(linear_regression, df_prepared_reduced, prices[train_set_index],\n",
    "                                           scoring=\"neg_mean_squared_error\", cv=K)\n",
    "linear_regression_rmse_scores = np.sqrt(-linear_regression_scores)\n",
    "display_scores(linear_regression_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22359858",
   "metadata": {},
   "source": [
    "### Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cac28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=14)\n",
    "forest_regressor.fit(df_prepared_reduced, prices[train_set_index])\n",
    "\n",
    "forest_regressor_prices_predictions = forest_regressor.predict(df_prepared_reduced)\n",
    "forest_regressor_mse = mean_squared_error(prices[train_set_index], forest_regressor_prices_predictions)\n",
    "forest_regressor_rmse = np.sqrt(forest_regressor_mse)\n",
    "forest_regressor_rmse.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a70cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_regressor_scores = cross_val_score(forest_regressor, df_prepared_reduced, prices[train_set_index],\n",
    "                                          scoring=\"neg_mean_squared_error\", cv=K)\n",
    "forest_regressor_rmse_scores = np.sqrt(-forest_regressor_scores)\n",
    "display_scores(forest_regressor_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(folder_path)\n",
    "# file_names : list = [i for i in glob.glob(\"*.{}\".format('csv'))]\n",
    "# df = pd.concat(map(read_car_data, file_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
